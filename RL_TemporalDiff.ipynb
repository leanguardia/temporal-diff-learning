{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "creWbxPTj5r2"
   },
   "source": [
    "# Temporal Difference\n",
    "Implement the basic TD learning algorithm (i.e. tabular TD(0)) to find a good path (given by a policy) for an agent exploring the environment from a starting point to a reward location in a simple environment.\n",
    "\n",
    "Where **S** is the starting location and **E** the end location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zTXjXynvj8DC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "WORLD = np.array([\n",
    "  [' ', ' ', ' ', ' ', ' ', ' ', ' '],\n",
    "  [' ', 'W', 'W', 'W', 'W', ' ', ' '], \n",
    "  [' ', 'W', 'E', ' ', 'W', ' ', ' '],\n",
    "  [' ', ' ', 'W', ' ', 'W', 'W', ' '],\n",
    "  [' ', ' ', 'W', ' ', ' ', ' ', ' '],\n",
    "  ['S', 'W', ' ', ' ', ' ', ' ', ' '],\n",
    " ])\n",
    "world_height = WORLD.shape[0]\n",
    "world_width = WORLD.shape[1]\n",
    "\n",
    "State = namedtuple('State', 'row col')\n",
    "initial_state = State(5, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reward(state):\n",
    "    return 1.0 if WORLD[state.row, state.col] == 'E' else 0.0\n",
    "\n",
    "reward(initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 \n",
      "0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 \n",
      "0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 \n",
      "0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 \n",
      "0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 \n",
      "0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 \n"
     ]
    }
   ],
   "source": [
    "def zero_values():\n",
    "    return np.zeros((WORLD.shape))\n",
    "\n",
    "def value(state):\n",
    "    return V[state.row, state.col]\n",
    "\n",
    "\n",
    "def show_values(decimal_numbers=5):\n",
    "    expression = \"{\" + \":.{}\".format(decimal_numbers) + \"f} \" \n",
    "    for row in range(V.shape[0]):\n",
    "        val = \"\" \n",
    "        for col in range(V.shape[1]):\n",
    "          val += expression.format(V[row, col])\n",
    "        print(val)\n",
    "        \n",
    "V = zero_values()\n",
    "# print(value(initial_state))\n",
    "show_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy\n",
    "- Deterministic: (e.g. always take the highest value) \n",
    "- Stochastic: (e.g. random exploration or epsilon-greedy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Action(Enum):\n",
    "    UP = 1\n",
    "    DOWN = 2\n",
    "    RIGHT = 3\n",
    "    LEFT = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVZnWke_lObf",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Action.UP: 1>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_wall(position):\n",
    "    return WORLD[position.row, position.col] == 'W'\n",
    "def can_go_left(state):\n",
    "    return state.col > 0 and not is_wall(State(state.row, state.col-1))\n",
    "def can_go_right(state):\n",
    "    return state.col < world_width-1 and not is_wall(State(state.row, state.col+1))\n",
    "def can_go_up(state):\n",
    "    return state.row > 0 and not is_wall(State(state.row-1, state.col))\n",
    "def can_go_down(state):\n",
    "    return state.row < world_height-1 and not is_wall(State(state.row+1, state.col))\n",
    "def possible_actions(state):\n",
    "    actions = []\n",
    "    if can_go_left(state):  actions.append(Action.LEFT)\n",
    "    if can_go_right(state): actions.append(Action.RIGHT)\n",
    "    if can_go_up(state):    actions.append(Action.UP)\n",
    "    if can_go_down(state):  actions.append(Action.DOWN)\n",
    "    return actions\n",
    "\n",
    "possible_actions(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hEijGMBg9y9Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action.RIGHT\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def go_up(state):\n",
    "    return State(state.row-1, state.col)\n",
    "def go_down(state):\n",
    "    return State(state.row+1, state.col)\n",
    "def go_left(state):\n",
    "    return State(state.row, state.col-1)\n",
    "def go_right(state):\n",
    "    return State(state.row, state.col+1)\n",
    "\n",
    "def take_action(prev_state, action):\n",
    "    if action is Action.UP: return go_up(prev_state)\n",
    "    if action is Action.DOWN: return go_down(prev_state)\n",
    "    if action is Action.LEFT: return go_left(prev_state)\n",
    "    if action is Action.RIGHT: return go_right(prev_state)\n",
    "\n",
    "def next_action(state):\n",
    "    actions = random.choice(possible_actions(state)) \n",
    "    action_to_values = {}\n",
    "    for action in possible_actions(state):\n",
    "        action_to_values[action] = value(take_action(state, action))\n",
    "    max_value = max(action_to_values.values())\n",
    "    max_actions = [a for a, v in action_to_values.items() if v == max_value]\n",
    "    return random.choice(max_actions)\n",
    "\n",
    "# print(take_action(State(5, 4), Action.UP))\n",
    "# print(next_action(initial_state))\n",
    "print(next_action(State(5, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_state(state):\n",
    "    return WORLD[state.row, state.col] == 'E'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0212748648 0.0290212385 0.0393151622 0.0529999480 0.0712040693 0.0954347309 0.1277029016 \n",
      "0.0154476112 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.1706904291 \n",
      "0.0110726432 0.0000000000 2.2852784539 1.7138114911 0.0000000000 0.0000000000 0.2279732185 \n",
      "0.0078036964 0.0000000000 0.0000000000 1.2851982894 0.0000000000 0.0000000000 0.3043190442 \n",
      "0.0053830300 0.0000000000 0.0000000000 0.9637242652 0.7226033800 0.5417459954 0.4060847635 \n",
      "0.0036163844 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 \n"
     ]
    }
   ],
   "source": [
    "V = zero_values()\n",
    "\n",
    "alpha = 0.5\n",
    "gamma = 0.75\n",
    "episodes = 50\n",
    "for episode in range(episodes):\n",
    "    state = initial_state\n",
    "    exploring = True\n",
    "    while(exploring):\n",
    "        action = next_action(state)\n",
    "        next_state = take_action(state, action)\n",
    "        learning = alpha * (reward(state) + gamma * value(next_state) - value(state))\n",
    "        V[state.row, state.col] += learning\n",
    "        if end_state(state): exploring = False\n",
    "        state = next_state\n",
    "show_values(decimal_numbers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "trvfcOrxm9vs"
   },
   "source": [
    "## Strategy learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['R', 'R', 'R', 'R', 'R', 'R', 'D'],\n",
       " ['U', ' ', ' ', ' ', ' ', 'R', 'D'],\n",
       " ['U', ' ', 'R', 'L', ' ', 'R', 'D'],\n",
       " ['U', 'L', ' ', 'U', ' ', ' ', 'D'],\n",
       " ['U', 'L', ' ', 'U', 'L', 'L', 'L'],\n",
       " ['U', ' ', 'R', 'U', 'U', 'U', 'U']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_encoding = {\n",
    "    Action.UP: 'U',\n",
    "    Action.DOWN: 'D',\n",
    "    Action.LEFT: 'L',\n",
    "    Action.RIGHT: 'R',\n",
    "}\n",
    "\n",
    "def learned_strategy():\n",
    "    strategy = []\n",
    "    for row in range(world_height):\n",
    "        s_row = []\n",
    "        for col in range(world_width):\n",
    "            if is_wall(State(row, col)):\n",
    "                s_row.append(\" \")\n",
    "            else:\n",
    "                action = next_action(State(row, col))\n",
    "                s_row.append(action_encoding[action])\n",
    "        strategy.append(s_row)\n",
    "    return strategy\n",
    "\n",
    "learned_strategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wr7Cj-1w99JL"
   },
   "outputs": [],
   "source": [
    "# count how many steps you require to get home\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yrfAHIYxm6Q7"
   },
   "outputs": [],
   "source": [
    "# Implement Epsylon - Greedy\n",
    "# Plot how V changes over learning\n",
    "# Plot how Policy changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RL-TemporalDiff.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
